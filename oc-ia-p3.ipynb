{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of OpenFoodFact dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade pingouin --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is made up of nutritional information on food items packages, that consumers input thanks to their compter or smartphone.\n",
    "\n",
    "Since it is an open source international data set, data may likely need cleaning, because of potential typos, non-homogenous label system, incomplete information, etc.\n",
    "\n",
    "After these cleaning steps, we'll dive deeper in the analysis of the data to understand their structure and potential correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import decomposition, preprocessing\n",
    "import pingouin\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import copy\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns = ['product_name', 'generic_name', 'categories', 'main_category', 'nutrition_grade_fr', 'ingredients_text',\n",
    "                    'energy_100g','proteins_100g','carbohydrates_100g', 'sugars_100g',\n",
    "                    'fat_100g', 'saturated-fat_100g', 'fiber_100g', 'sodium_100g']\n",
    "labels = dict(zip(['energy_100g','proteins_100g','carbohydrates_100g', 'sugars_100g',\n",
    "                    'fat_100g', 'saturated-fat_100g', 'fiber_100g', 'sodium_100g'], \n",
    "                 ['Energy', 'Proteins', 'Carbohydrates', 'Sugars', 'Fat', 'Saturated fat', 'Fibers', 'Sodium']))\n",
    "quantitative_columns = [col for col in labels.keys() if '100g' in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('products.csv', sep='\\t', usecols=interesting_columns)[interesting_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()\n",
    "original_size = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "# 1. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## 1.1 Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The dataset contains {data.shape[0]} rows and {data.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Here is an excerpt of the dataset:<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Let's see the main characteristics.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "data.describe(include='all').style.format(dict.fromkeys(quantitative_columns, '{:.2f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## 1.2 Cleaning columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Column 'categories'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>The '**categories**' column has much more unique values than the '**main_category**' and very similar number of missing values.  \n",
    "So we can drop this column that is less relevant than \"main_category\".\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_of_null_values = pd.DataFrame(data.isnull().sum()/len(data), columns = [\"Missing values\"])\n",
    "percentage_of_null_values.sort_values(by='Missing values', ascending=False, inplace=True)\n",
    "percentage_of_null_values.style.format({\"Missing values\": '{:.2%}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Column 'generic_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>The column '**generic_name**' has more than 83% of missing values.  \n",
    "We may drop it, but beforehand, use its values to fill the 'product_name' value when missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_product_name_but_generic_name = data[data.product_name.isna() & data.generic_name.notna()]\n",
    "data.loc[no_product_name_but_generic_name.index, 'product_name'] = data.loc[no_product_name_but_generic_name.index, 'generic_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Check on the first 5 rows that the values were properly replaced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[no_product_name_but_generic_name.index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Now we can drop the 'generic_name' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['generic_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "## 1.3 Cleaning rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Duplicate values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class provides a tool to plot repeatedly a pie chart that includes a new portion each time.\n",
    "\n",
    "class ProgressivePie:\n",
    "    def __init__(self, title, original_size, colors):\n",
    "        self.original_size = original_size\n",
    "        self.colorset = colors\n",
    "        self.title = title\n",
    "        self.existing_labels = []\n",
    "        self.values = [self.original_size]\n",
    "        self.labels = [\"\"]\n",
    "        self.colors = [self.colorset[0]]\n",
    "\n",
    "    def _increment_data(self, new_value, new_label):\n",
    "        self.values = [new_value] + self.values[:-1] + [self.values[-1] - new_value]\n",
    "        self.labels = [new_label] + self.labels[:-1] + [f'{self.values[-1]} rows']\n",
    "        self.colors = [self.colorset[len(self.colors)%len(self.colorset)]] + self.colors\n",
    "\n",
    "    def _plot_pie_chart(self):\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.pie(self.values, labels=self.labels, explode=[0.1]+[0]*(len(self.values)-1), autopct='%1.1f%%', colors=self.colors)\n",
    "        ax1.set_title(self.title + f' \\n(total number of lines = {self.original_size})')\n",
    "        plt.show()\n",
    "\n",
    "    def plot(self, rows, label):\n",
    "        num_rows = rows.shape[0]\n",
    "        if label in self.existing_labels:\n",
    "            self._plot_pie_chart()\n",
    "        else:\n",
    "            self._increment_data(num_rows, f'{label.capitalize()} ({num_rows} rows)')\n",
    "            self._plot_pie_chart()\n",
    "            self.existing_labels.append(label)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = ProgressivePie(original_size=data.shape[0],\n",
    "                    title='Percentage of droppable values in dataset',\n",
    "                    colors=['tab:green', 'tab:red', 'tab:cyan', 'tab:orange', 'tab:blue', 'tab:brown', 'tab:purple', 'tab:olive', 'tab:gray', 'tab:pink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>How many values are duplicates ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_lines = data[data.duplicated()]\n",
    "pp.plot(duplicate_lines, \"duplicate lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(duplicate_lines.index)\n",
    "print(f'After dropping duplicate values, dataframe contains {data.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Missing data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of analysis, we will need complete quantitative data.  \n",
    "Partially filled lines may exist since a food item does not always contain all nutrients, but lines with no quantitative data at all can be dropped.  \n",
    "What proportion do they represent?<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_lines = data[data[quantitative_columns].isna().all(axis=1)]\n",
    "pp.plot(missing_lines, \"lines missing all quantitative values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(missing_lines.index)\n",
    "print(f'After dropping rows with no quantitative data, dataframe contains {data.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Remaining lines are supposed to have at least one relevant quantitative value.The more we have, the better. But they are not equally important.  \n",
    "\n",
    "a) The most important data are '**proteins_100g**', '**carbohydrates_100g**' and '**fat_100g**'. They are essential, so any NaN value in these fields result in dropping the row.\n",
    "\n",
    "b) '**energy_100g**' can be inferred from the previous 3 values (energy = 37 \\* fat + 17 \\* proteins + 17 \\* sugar). \n",
    "\n",
    "c) '**sugars_100g**' and '**saturated_fat_100g**' are subset of 'carbohydrates_100g' and 'fat_100g' respectively. And as for '**fiber_100g**', '**sodium_100g**', they are interesting information to perform a more detailed analysis but are not essential. To unify the datatype, we will transform in 0 (zero) values all NaN values for these columns.\n",
    "\n",
    "Let's process the data successively, according to these rules.<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.a Dropping data with missing essential values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_essential_lines = data[data[['proteins_100g', 'carbohydrates_100g', 'fat_100g']].isna().any(axis=1)]\n",
    "pp.plot(missing_essential_lines, \"lines missing essential values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(missing_essential_lines.index)\n",
    "print(f'After dropping rows with missing essential values, dataframe contains {data.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.b Replacing NaN values in \"energy_100g\" by (37 \\* fat + 17 \\* proteins + 17 \\* carbohydrates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>This operation does not change the total lines number.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_nan = data[data['energy_100g'].isna()]\n",
    "def calc_energy(row): \n",
    "    return row['fat_100g'] * 37 + row['proteins_100g'] * 17 + row['carbohydrates_100g'] * 17\n",
    "\n",
    "data['energy_100g'] = data['energy_100g'].fillna(value = calc_energy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.c Replacing NaN values by 0 in other quantitative columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Again, no line is deleted in this step.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,quantitative_columns] = data.loc[:,quantitative_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is no NaN any more in quantitative columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Incorrect data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Let's look at central tendencies and dispersions for quantitative data. <br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().style.format(dict.fromkeys(quantitative_columns,'{:.2f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>How can we identify and handle inconsistent values?\n",
    "\n",
    "1. Except for 'energy_100g', the values correspond to the number of **grams** of each nutrient **in 100 grams** of considered food item. **Values should then be comprised between 0 and 100**. We observe that some nutrients present negative minima, and all values have maxima much higher. These data are to be dropped. \n",
    "\n",
    "2. **The value of saturated fat cannot exceed that of global fat**. Same case for **sugars** with respect to carbohydrates.\n",
    "\n",
    "3. **The sum of fat, carbohydrates and proteins cannot exceed 100 grams** [(source)](https://fr.openfoodfacts.org/questions-frequentes). Rows that do not fulfill this condition should be dropped as well.\n",
    "\n",
    "4. Regarding energy, its **highest possible value (case of pure vegetal oil) is 3700 kJ** or 900 kcal [(source)](https://en.wikipedia.org/wiki/Food_energy). The unit is not specified in OpenFoodFact documentation, so we'll use the highest value, that is 3700 kJ, especially as the kJ is the official unit. Hence, any row with energy value beyond this limit or below 0 will be dropped.\n",
    "\n",
    "All these tests can be grouped in a function to be applied in one go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to identify rows that do not fulfill all criteria above\n",
    "\n",
    "def irrelevant(row, max_energy=3700, max_weight=100, energy_column='energy_100g',\n",
    "                list_of_cols_to_sum = ['proteins_100g', 'fat_100g', 'carbohydrates_100g'],\n",
    "                nutrient_columns = [col for col in quantitative_columns if col != 'energy_100g']):\n",
    "    return (\n",
    "        row[energy_column] > max_energy or\n",
    "        (row[nutrient_columns] > 100).any() or\n",
    "        (row[quantitative_columns] < 0).any() or\n",
    "        np.floor(sum(row[list_of_cols_to_sum])) > max_weight or\n",
    "        row['saturated-fat_100g'] > row['fat_100g'] or\n",
    "        row['sugars_100g'] > row['carbohydrates_100g']\n",
    "    )\n",
    "\n",
    "# Create examples for different cases:\n",
    "\n",
    "row_ok = pd.Series(data={'product_name': 'biscuit', \"generic_name\": None, \"main_category\": None, \n",
    "                         'nutrition_grade_fr': None, 'ingredients_text': \"some ingredients\",\n",
    "                         'energy_100g': 1000,'proteins_100g':13.2, 'carbohydrates_100g':55.8,\n",
    "                         'sugars_100g': 12, 'fat_100g':30.4, 'saturated-fat_100g': 20.5,\n",
    "                         'fiber_100g':3, 'sodium_100g':4}, name=\"row_ok\")\n",
    "row_wrong_energy = copy.deepcopy(row_ok)\n",
    "row_excess_value = copy.deepcopy(row_ok)\n",
    "row_neg_value = copy.deepcopy(row_ok)\n",
    "row_wrong_sum = copy.deepcopy(row_ok)\n",
    "row_wrong_sat_fat = copy.deepcopy(row_ok)\n",
    "row_wrong_sugar = copy.deepcopy(row_ok)\n",
    "\n",
    "row_wrong_energy['energy_100g'] = 5000\n",
    "row_excess_value['carbohydrates_100g'] = 120\n",
    "row_neg_value['sodium_100g'] = -1\n",
    "row_wrong_sum['proteins_100g'] = 95\n",
    "row_wrong_sat_fat['saturated-fat_100g'] = 31\n",
    "row_wrong_sugar['sugars_100g'] = 60\n",
    "\n",
    "# Testing\n",
    "\n",
    "names = ['row ok', \n",
    "         'row with negative value',\n",
    "         'row with excessive value', \n",
    "         'row with wrong sum', \n",
    "         'row with wrong energy', \n",
    "         'row_wrong_sat_fat', \n",
    "         'row_wrong_sugar']\n",
    "rows = [row_ok, \n",
    "        row_neg_value,\n",
    "        row_excess_value, \n",
    "        row_wrong_sum, \n",
    "        row_wrong_energy, \n",
    "        row_wrong_sat_fat, \n",
    "        row_wrong_sugar]\n",
    "results = [False, True, True, True, True, True, True]\n",
    "\n",
    "tests_OK = not irrelevant(row_ok) and all([irrelevant(row) for row in rows[1:]])\n",
    "if not tests_OK:\n",
    "    print('Tests failed, please check')\n",
    "    for name, row, result in zip(names, rows, results):\n",
    "        if result != irrelevant(row):\n",
    "            print(f'Testing {name}, expect function to return {result}, result: ',irrelevant(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the dataset (the process takes up to 7-9 min on Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(\"Cleaning started at\", start.strftime('%H:%M:%S'))\n",
    "data['irrelevant'] = data.apply(irrelevant, axis=1)\n",
    "end = datetime.now()\n",
    "print('Ended at', end.strftime('%H:%M:%S'))\n",
    "print('Elapsed time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_lines = data[data['irrelevant']]\n",
    "pp.plot(irrelevant_lines, 'irrelevant lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(irrelevant_lines.index)\n",
    "data = data.drop('irrelevant', axis=1)\n",
    "print(f'After dropping rows where values are irrelevant, dataframe contains {data.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = (original_size - data.shape[0])/original_size\n",
    "print(f\"After this cleaning session, {'{:.2%}'.format(dropped)} of initial data have been dropped for being duplicate, incomplete or irrelevant.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>To improve readability in graphs, we'll also replace by '' all NaN fields in qualitative columns and shorten ingredient list to 30 characters.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative_columns = [col for col in data.columns if col not in quantitative_columns]\n",
    "data.fillna(dict.fromkeys(qualitative_columns, ''), inplace=True)\n",
    "data['ingredients_text']= data['ingredients_text'].str.slice(0,30)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Now we can visualize statistics on our cleaned dataset.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().style.format(dict.fromkeys(quantitative_columns, '{:.2f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>To visualize these figures, we'll use boxplots, that show on the same graph the median, quartiles and outliers (i.e. values that are distant to the median for more than 1,5 * the interquartile distance).<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2*4 grid for subplots\n",
    "\n",
    "rows, cols = 2, 4\n",
    "fig = make_subplots(rows=rows, cols=cols)\n",
    "\n",
    "# Plot boxplots for all quantitative columns\n",
    "\n",
    "grad = list(itertools.product(range(1,rows+1), range(1, cols+1)))\n",
    "for (row, col), variable in zip(grad, quantitative_columns):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data[variable], name=labels[variable], boxmean=True),\n",
    "        row=row, col=col)\n",
    "fig.update_layout(height=800, width=1000,\n",
    "                  title_text=\"Nutrients quantity per 100g\", showlegend=False)    \n",
    "fig.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>The boxplots are \"squashed\" because of many zero or outliers values, especially for sodium. Let's look at those values more precisely, by displaying only products where the values are > 0.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, color in zip(quantitative_columns, px.colors.qualitative.Plotly):\n",
    "    partial_data = data[data[col]>0][['product_name', 'ingredients_text', col]]\n",
    "\n",
    "    fig = px.histogram(partial_data, x=col, nbins=100, color_discrete_sequence=[color],\n",
    "                       marginal=\"box\", hover_data=['product_name', 'ingredients_text'])\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "# 2. Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "## 2.1. Matrix of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlations = data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(correlations, color_continuous_scale = 'Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>We can see that energy is highly correlated to fat, carbohydrates and proteins, and that:  \n",
    "- carbohydrates and sugars, on one hand,  \n",
    "- fat and saturated fat, on the other hand,  \n",
    "are closely related.\n",
    "We can also notice that sodium is in no way related to other nutrients.<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must keep in mind that the main nutrients (fat, carbohydrates and proteins) are correlated, since their sum is limited to 100. This can be seen if we plot food items in a 3D space based on these 3 nutrients vectors. We get a tetrahedron (plot made from a sample od 50 000 food items):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(sample, x='fat_100g', y='carbohydrates_100g', z='proteins_100g', color=\"nutrition_grade_fr\",\n",
    "             category_orders = {'nutrition_grade_fr': ['a', 'b', 'c', 'd', 'e']}\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## 2.2. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>We'll use a Principal Components Analysis to determine which factors, among the quantitative values of the dataset, describe it best.  \n",
    "We'll exclude the energy value, since it is dependant of other values (made up of fat, carbohydrates and proteins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "nutrient_columns = [col for col in quantitative_columns if col != 'energy_100g']\n",
    "n_comp = len(nutrient_columns)\n",
    "data_pca = data[nutrient_columns]\n",
    "X = data_pca.values\n",
    "names = data_pca.index\n",
    "features = data_pca.columns\n",
    "\n",
    "# Normalize values\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "\n",
    "pca = decomposition.PCA(n_components=n_comp)\n",
    "pca_data = pca.fit(X_scaled)\n",
    "pc_index = [f'PC{i+1}' for i in range(n_comp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var = pd.DataFrame(\n",
    "    np.cumsum(pca_data.explained_variance_ratio_), \n",
    "    columns = ['Cumulative explained variance'], \n",
    "    index = pc_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = cum_var.index\n",
    "x.name = \"Components\"\n",
    "fig = px.line(cum_var, \n",
    "              x=x, \n",
    "              y=\"Cumulative explained variance\", \n",
    "              title='Cumulative explained variance',\n",
    "              hover_data={'Cumulative explained variance': ':.2%'}\n",
    "             )\n",
    "fig.update_traces(mode=\"markers+lines\")\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>We observe that the first 2 components explain only just more than 50%, and 4 components explain more 80% of the variance. More specifically:<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components = pd.DataFrame(data=pca_data.components_, columns=nutrient_columns, index = pc_index)\n",
    "complete = principal_components.copy()\n",
    "complete['Explained variance'] = pca_data.explained_variance_ratio_\n",
    "complete['Cumulative explained variance'] = cum_var['Cumulative explained variance']\n",
    "\n",
    "complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadings = principal_components.T* np.sqrt(pca_data.explained_variance_)\n",
    "\n",
    "fig= fig = go.Figure(\n",
    "    layout_title_text=\"Nutrients represented in the first vectorial plan (P1, P2)\",\n",
    ")\n",
    "fig.update_xaxes(range=[-1, 1])\n",
    "fig.update_yaxes(range=[-1, 1])\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800,)\n",
    "\n",
    "for (i, nutrient), color in zip(enumerate(nutrient_columns), px.colors.qualitative.Plotly):\n",
    "    fig.add_shape(\n",
    "        type='line',\n",
    "        x0=0, y0=0,\n",
    "        x1=loadings.iloc[i, 0],\n",
    "        y1=loadings.iloc[i, 1],\n",
    "        line=dict(color=color,width=2)\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=loadings.iloc[i, 0],\n",
    "        y=loadings.iloc[i, 1],\n",
    "        ax=0, ay=0,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"bottom\",\n",
    "        text=nutrient,\n",
    "    )\n",
    "    \n",
    "fig.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect, we notice that sugars and carbohydrates are very close, as well as fat and saturated fat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## 2.3 ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nutriscore was created to reflect the overall \"quality\" of food items. Beyond the only caloric intake, it is aimed to take into account the levels of sodium, fibers, saturated fats, sugar, etc.  \n",
    "\n",
    "Here we are going to analyze how the nutriscore, which is a qualitative value scoring from a to e, has an influence on quantitative values. By doing this we may be able to make hypothesis on the definition of the nutriscore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutriscore = data[data['nutrition_grade_fr']!=\"\"]\n",
    "nutriscore.groupby(\"nutrition_grade_fr\").mean().style.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>We can see there is a \"correlation\" between nutrition grade and some nutrients, but since nutrition grade is a qualitative value, this correlation has to be studied through an Analysis of Variance (ANOVA.)<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in quantitative_columns:\n",
    "    fig = px.box(nutriscore,x='nutrition_grade_fr', y=item, \n",
    "                 title=f'Distribution of {item} for each nutriscore value',\n",
    "                 labels={'nutrition_grade_fr': \"Nutriscore\"}, \n",
    "                 color = 'nutrition_grade_fr',\n",
    "                 category_orders={'nutrition_grade_fr': [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n",
    "                )\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Since the fibers and sodium values are the one less visible in the boxplot, because of the outliers, we hope that performing ANOVA will allow us to check whether they depend on nutriscore.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize nutriscore dataframe\n",
    "\n",
    "to_normalize = nutriscore[quantitative_columns].values\n",
    "scaler = preprocessing.StandardScaler().fit(to_normalize)\n",
    "normalized = scaler.transform(to_normalize)\n",
    "nutriscore_norm = nutriscore.copy()\n",
    "nutriscore_norm[quantitative_columns] = normalized\n",
    "\n",
    "# Create a dataframe to stack results\n",
    "\n",
    "columns = ['item', 'F', 'np2']\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "for item in quantitative_columns:\n",
    "    anova =  pingouin.anova(data=nutriscore_norm, dv=item, between='nutrition_grade_fr')\n",
    "    row = pd.DataFrame(np.array([[item, anova.loc[0,'F'],anova.loc[0, 'np2']]]), columns=columns)\n",
    "    results = pd.concat([results, row])\n",
    "\n",
    "# Format and sort dataframe\n",
    "\n",
    "results.loc[:,['F', 'np2']] = results.loc[:,['F', 'np2']].astype(float)\n",
    "results.sort_values(by=\"F\", ascending=False, inplace=True)\n",
    "results.reset_index(inplace=True)\n",
    "results.drop('index', axis=1, inplace = True)\n",
    "results.style.format({'F': '{:.2f}', 'np2':'{:.2f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The F score represents the variance interclass divided by the variance intraclass. A hig score shows that there are much more variations between classes than in each class, that is to say, the different classes do have an impact. \n",
    "- The eta-square (np2) score represents the variance interclass divided by the overall variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>The chart above shows that:  \n",
    "- the nutrition score is a relevant parameter with respect to saturated fat, energy, fat and sugar  \n",
    "- on the other hand, the content in carbohydrates, fibers, proteins and sodium is little or very little influenced by the nutriscore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences on fat, saturated fat and sugars come as no suprise given the concept of Nutriscore. Neither the small score of proteins that is neither a \"bad\" or \"good\" nutrient.  \n",
    "But the small differences on fibers and sodium are surprising. We could have thought that they would be more strongly related to the nutriscore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may conclude that the relative importances of fat and sugar in nutriscore are higher than that of sodium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another explanation could be that while it is **sodium** that is Monitored by nutriscore, some labels mention the **salt**. Howerver, for 100g of salt, there is only 40g of sodium [(source)](https://www.nephrohug.ch/2016/04/20/le-sel-le-sodium-quelle-difference/). And not all food labels specifiy \"sodium\" : many of them mention \"salt\". If consumers mention the salt rate instead of the sodium rate in the database, then it can skew the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also be aware of the fact that nutriscores are no represented equally in the database. The highest scores c, d and e are overrepresented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(data, x=\"nutrition_grade_fr\",\n",
    "             color = 'nutrition_grade_fr',\n",
    "             category_orders={'nutrition_grade_fr': ['', 'a', 'b', 'c', 'd', 'e']},\n",
    "             labels= {'':\"no score\", \"a\": \"A\", \"b\": \"B\", \"c\": \"C\", \"d\": \"D\", \"e\":\"E\"},  \n",
    "             title='distribution of nutrition grade',\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
